# VALL-E-X: Zero-Shot Text-to-Speech Synthesis API (English Only)

VALL-E-X es una implementaci√≥n de s√≠ntesis de voz de alta calidad que puede generar habla personalizada con solo una grabaci√≥n de 3 segundos de un hablante desconocido como prompt ac√∫stico. Esta implementaci√≥n est√° optimizada espec√≠ficamente para **s√≠ntesis de texto a voz en ingl√©s**.

## üéØ Caracter√≠sticas Principales

- **Zero-shot voice cloning**: Clona cualquier voz con solo 3-10 segundos de audio
- **S√≠ntesis en ingl√©s**: Optimizado espec√≠ficamente para el idioma ingl√©s
- **API REST con FastAPI**: Interfaz HTTP f√°cil de usar para integraci√≥n
- **Transcripci√≥n autom√°tica**: Usa Whisper para extraer texto del audio prompt
- **Compatibilidad multiplataforma**: Funciona en Windows, Linux y macOS con soporte CPU/CUDA/MPS

## üèóÔ∏è Arquitectura del Proyecto

### üìÅ Estructura de Directorios

```
VALL-E-X/
‚îú‚îÄ‚îÄ app.py                 # API principal con FastAPI
‚îú‚îÄ‚îÄ test_app.py           # Tests unitarios de la API  
‚îú‚îÄ‚îÄ macros.py             # Configuraciones y constantes globales
‚îú‚îÄ‚îÄ requirements.txt      # Dependencias del proyecto
‚îú‚îÄ‚îÄ data/                 # M√≥dulos de procesamiento de datos
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py     # Tokenizaci√≥n de audio con EnCodec
‚îÇ   ‚îî‚îÄ‚îÄ collation.py     # Procesamiento de tokens de texto
‚îú‚îÄ‚îÄ models/              # Arquitectura del modelo neural
‚îÇ   ‚îî‚îÄ‚îÄ vallex.py       # Modelo principal VALL-E
‚îú‚îÄ‚îÄ utils/               # Utilidades del sistema
‚îÇ   ‚îî‚îÄ‚îÄ g2p/            # Conversi√≥n grafema a fonema
‚îÇ       ‚îî‚îÄ‚îÄ bpe_69.json # Tokenizer BPE para fonemas
‚îú‚îÄ‚îÄ checkpoints/         # Pesos del modelo pre-entrenado (descarga autom√°tica)
‚îú‚îÄ‚îÄ whisper/            # Modelos Whisper para transcripci√≥n (descarga autom√°tica)
‚îú‚îÄ‚îÄ output/             # Archivos de audio generados por la API
‚îú‚îÄ‚îÄ prompts/            # Prompts de audio temporales para procesamiento
‚îî‚îÄ‚îÄ nltk_data/          # Datos de NLTK (si es necesario)
```

## üß† Componentes Principales

### 1. **API Principal (`app.py`)**

El coraz√≥n del sistema es una API REST construida con FastAPI que proporciona:

**Endpoints principales:**
- **`/api/infer_audio/`**: Genera voz sint√©tica a partir de un prompt de audio
- **`/api/ping`**: Verificaci√≥n de estado de la API  
- **`/api/test-audio-access/{filename}`**: Test de acceso a archivos generados
- **`/audio/*`**: Servir archivos de audio est√°ticos

**Funcionalidades t√©cnicas:**
- **Configuraci√≥n autom√°tica del dispositivo**: CPU, CUDA o MPS (Apple Silicon)
- **Descarga autom√°tica de modelos**: VALL-E-X checkpoint y Whisper medium
- **Transcripci√≥n con Whisper**: Extracci√≥n autom√°tica de texto del audio prompt
- **Tokenizaci√≥n dual**: Audio (EnCodec) y texto (BPE fon√©tico)
- **S√≠ntesis VALL-E**: Generaci√≥n autoregresiva y no-autoregresiva
- **Decodificaci√≥n Vocos**: Conversi√≥n de tokens a audio de alta calidad (24kHz)
- **Gesti√≥n de memoria**: Descarga autom√°tica de modelos cuando no se usan
- **Parche multiplataforma**: Soluci√≥n para PathLib en sistemas Unix

**Configuraci√≥n CORS:**
```python
allow_origins=["https://pronunciapp.me", "https://www.pronunciapp.me", "http://localhost:4200"]
```

### 2. **Configuraci√≥n Global (`macros.py`)**

Define las constantes y configuraciones esenciales del sistema:

```python
# Arquitectura del modelo
NUM_LAYERS = 12        # Capas del transformer
NUM_HEAD = 16          # Cabezas de atenci√≥n
N_DIM = 1024          # Dimensi√≥n del modelo
NUM_QUANTIZERS = 8     # Niveles de cuantizaci√≥n
SAMPLE_RATE = 24000    # Frecuencia de muestreo

# Configuraci√≥n simplificada para ingl√©s √∫nicamente
LANGUAGE = "en"
LANGUAGE_TOKEN = "[EN]"
```

### 3. **Funciones Core del Sistema (`app.py`)**

#### **Transcripci√≥n de Audio (`transcribe_one`)**
Utiliza Whisper para transcribir audio y detectar idioma:
```python
def transcribe_one(model, audio_path):
    # Carga y procesa el audio
    # Detecta idioma autom√°ticamente
    # Transcribe el contenido
    # A√±ade puntuaci√≥n si es necesario
```

#### **Creaci√≥n de Prompts (`make_prompt`)**
Procesa archivos de audio para crear prompts de voz:
```python
def make_prompt(name, wav, sr, save=True):
    # Normaliza el audio (mono, rango [-1,1])
    # Transcribe con Whisper
    # A√±ade tokens de idioma
    # Guarda archivos temporales
```

#### **Inferencia Principal (`infer_from_audio`)**
Funci√≥n central que realiza la s√≠ntesis de voz:
```python
def infer_from_audio(text, language, accent, audio_prompt, record_audio_prompt, transcript_content):
    # Procesa el audio de entrada
    # Tokeniza audio y texto
    # Ejecuta inferencia con modelo VALL-E
    # Decodifica con Vocos
    # Gestiona memoria de GPU
```

### 4. **Tests del Sistema (`test_app.py`)**

El proyecto incluye una suite completa de tests unitarios:

```python
def test_ping():                           # Verifica conectividad de la API
def test_infer_audio_no_file():           # Valida manejo de errores
def test_infer_audio_with_dummy_wav():    # Test completo de s√≠ntesis
def test_audio_access_endpoint():         # Verifica acceso a archivos generados
```

**Caracter√≠sticas de testing:**
- Tests autom√°ticos con archivos de audio sint√©ticos
- Validaci√≥n de endpoints y manejo de errores
- Verificaci√≥n de generaci√≥n y acceso a archivos
- Limpieza autom√°tica de archivos temporales

## üöÄ Instalaci√≥n y Uso

### Requisitos del Sistema
- Python 3.7+
- PyTorch (con soporte CUDA opcional)
- FFmpeg para procesamiento de audio

### Instalaci√≥n

1. **Clonar el repositorio:**
```bash
git clone <tu-repositorio>
cd VALL-E-X
```

2. **Instalar dependencias:**
```bash
pip install -r requirements.txt
```

3. **Crear directorios necesarios:**
```bash
mkdir -p checkpoints whisper output prompts
```

### Uso de la API

1. **Iniciar el servidor:**
```bash
python app.py
# o usar uvicorn directamente:
uvicorn app:app --host 0.0.0.0 --port 8000
```

2. **Hacer una petici√≥n de s√≠ntesis:**
```bash
curl -X POST "http://localhost:8000/api/infer_audio/" \
  -F "text_input=Hello, this is a test of voice synthesis" \
  -F "upload_audio_prompt=@your_voice_sample.wav"
```

3. **Respuesta esperada:**
```json
{
  "text_output": "text prompt: [detected_text]\nsynthesized text: [EN]Hello, this is a test[EN]",
  "audio_url": "https://your-domain.com/audio/generated_audio.wav",
  "audio_data": "base64_encoded_audio_data"
}
```

## üß™ Testing

Ejecutar tests unitarios:

```bash
# Ejecutar todos los tests
pytest test_app.py -v

# Tests espec√≠ficos
pytest test_app.py::test_ping           # Test de conectividad
pytest test_app.py::test_infer_audio    # Test de s√≠ntesis
```

## üîß Configuraci√≥n Avanzada

### Variables de Entorno
```python
# Configuraci√≥n autom√°tica de dispositivo
device = "cuda" if torch.cuda.is_available() else "cpu"
device = "mps" if torch.backends.mps.is_available() else device

# Configuraci√≥n de hilos
thread_count = multiprocessing.cpu_count()
torch.set_num_threads(thread_count)
```

### Descarga Autom√°tica de Modelos
- El modelo VALL-E-X se descarga autom√°ticamente desde HuggingFace
- Whisper se descarga al primer uso
- Los modelos se almacenan en `./checkpoints/` y `./whisper/`

## üìä Flujo de Procesamiento

1. **Input**: Audio prompt (3-10 segundos)
2. **Transcripci√≥n**: Whisper extrae texto del audio prompt
3. **Tokenizaci√≥n**: 
   - Texto ‚Üí Fonemas ‚Üí Tokens
   - Audio ‚Üí EnCodec ‚Üí Tokens cuantizados
4. **Inferencia**: 
   - Modelo AR genera primer quantizer
   - Modelo NAR genera quantizers restantes
5. **Decodificaci√≥n**: Vocos convierte tokens a audio
6. **Output**: Audio sint√©tico con la voz del prompt

## üåê Soporte de Idioma

Esta implementaci√≥n est√° optimizada espec√≠ficamente para **ingl√©s**:

| Idioma | C√≥digo | Token | Estado |
|--------|--------|-------|--------|
| Ingl√©s | `en` | `[EN]` | ‚úÖ Soportado |

### Simplificaci√≥n para Ingl√©s
- Eliminaci√≥n de detecci√≥n autom√°tica de idioma
- Tokens de idioma simplificados
- Procesamiento optimizado para fonemas en ingl√©s
- Reducci√≥n del tama√±o del modelo al eliminar embeddings multiling√ºes

## üìà Rendimiento y Optimizaciones

- **Gesti√≥n de memoria**: Descarga autom√°tica de modelos de GPU cuando no se usan
- **Cache KV**: Optimizaci√≥n para generaci√≥n secuencial en modo AR
- **Paralelizaci√≥n**: Soporte para m√∫ltiples trabajadores de datos
- **Compatibilidad de hardware**: CPU, CUDA, y Apple Silicon (MPS)
- **Parches de compatibilidad**: Soporte para sistemas Unix/Linux/macOS

## üîç Debugging y Monitoreo

### Logs del Sistema
- Informaci√≥n detallada sobre carga de modelos
- M√©tricas de rendimiento y uso de memoria

### Endpoints de Diagn√≥stico
- `/api/ping`: Verificaci√≥n de estado de la API
- `/api/test-audio-access/{filename}`: Verificaci√≥n de acceso a archivos

## üìù Estructura de Datos

### Directorios de Trabajo
- `./checkpoints/`: Modelo VALL-E-X pre-entrenado
- `./whisper/`: Modelos Whisper para transcripci√≥n
- `./output/`: Audio generado por la API
- `./prompts/`: Archivos temporales de procesamiento

---

## üôè Agradecimientos

Este proyecto es una implementaci√≥n basada en el excelente trabajo de [VALL-E-X](https://github.com/Plachtaa/VALL-E-X) desarrollado por Plachtaa, el cual a su vez est√° basado en el trabajo de investigaci√≥n VALL-E-X de Microsoft Research. Se agradece a ambos equipos por su contribuci√≥n al desarrollo de tecnolog√≠as de s√≠ntesis de voz de c√≥digo abierto.

**Nota**: Este proyecto implementa el paper "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers" con mejoras espec√≠ficas para producci√≥n y soporte optimizado para ingl√©s.

